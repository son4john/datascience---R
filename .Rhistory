lm.out = lm(GPM~., data=FuelEff)
lm.out
summary(lm.out)
vif(lm.out)
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(lm.out,which=1:4)
library(leaps)
X=FuelEff[,2:7]
y=FuelEff[,1]
out=summary(regsubsets(X,y,nbest=2,nvmax=ncol(X)))
tab=cbind(out$which,rsq=out$rsq,adjr2=out$adjr2,cp=out$cp,bic=out$bic)
tab
n=length(FuelEff$GPM)
diff=dim(n)
percdiff=dim(n)
dim(fuellEff)
dim(FuelEff)
detach(FuelEff)
library(MASS)
library(corrplot)
library(Hmisc)
library(leaps)
toyota = read.csv("ToyotaCorolla.csv")
toyota = read.csv("D:/DRIVES/OneDrive/EDU/FIN_6368/code/ToyotaCorolla.csv")
head(toyota)
plot(toyota)
#turn a catagorical variable into dummy variables
#3 states/ level need two variables (0,0), (1,0), (0,1)
toyota$FuelType1=ifelse(toyota$FuelType=="CNG",1,0) #include new variable to data
toyota$FuelType2=ifelse(toyota$FuelType=="Diesel",1,0) #include new variable to data
auto=toyota[-4] #remove orignal FuelType from orignal data and store in new variable
head(auto)
m1=lm(Price~., data=auto) #lookng at how all the other variables ~. are related to price
summary(m1) #PR(>|t|) is the p-values
x=auto[,2:11] #independent
y=auto[,1] #dependent
out=summary(regsubsets(x,y,nbest=1,nvmax=ncol(x))) #nbest select the n best model 1
tab=cbind(out$which,rsq=out$rsq,adjr2=out$adjr2,cp=out$cp,bic=out$bic) #evaluate the model using rsqared cp bic adjusted rsquare
tab # if using one figure use age, if using two figures use age KM bic smallest
n=length(auto$Price)
n1=round(n*.6)
n2=n-n1
n2=n-n1
train=sample(1:n,n1)
m1=lm(Price~Age+KM+HP+CC+Weight+FuelType1+FuelType2,data=auto[train,])
summary(m1)
pred=predict(m1,newdata = auto[-train,])
obs=auto$Price[-train]
diff1 = obs - pred
percdiff1 = abs(diff1)/obs
length(obs)
length(pred)
length(train)
me1=mean(diff1)
rmse1=sqrt(mean(diff1**2))
mape1=100*(mean(percdiff1))
me1
rmse1
mape1
x = seq(0,.5,.01) # is a vector starting at 0 going to .5 with a stepsize of .01
y1 = 1-x #y1 is 1 minus the value of x
y2 = exp(-x) #is the exponetal value of -x
plot(x,y1,type="l",ylab="1/x,exp(-x)",col=2)
lines(x,y2,type="l",col=8)
S = c(1.3,1.2,1.3,1.4,1.5,1.4,1.3,1.4,1.5) #vector with 9 values
#lag value
diffLogS = diff(log(S)) #get log off all s then current minus previous, vector with 8 values
diffLogSmean = mean(diffLogS)
N = length(diffLogS) #total number of returns
histVol = sqrt(1/(N-1)*sum((diffLogS-diffLogSmean)^2))#historical volatility pg 35 equation 3.30
annHistVol = histVol*sqrt(length(S))#annualizing similar to the above comment pg 35 equation 3.32
histVol
annHistVol
install.packages("RSQLite")#everytime i closed the browser it would delete it
library(RSQLite)#sql libaray
library(foreign)#functions to make calls to compiled code that has been loaded into R
setwd("D:/DRIVES/OneDrive/EDU/FIN_6368")
funda <- read.dta("funda.dta") #data1
msf   <- read.dta("msf.dta") #data2
setwd("D:/DRIVES/OneDrive/EDU/FIN_6368/code")
funda <- read.dta("funda.dta") #data1
msf   <- read.dta("msf.dta") #data2
con   <- dbConnect(SQLite(),":memory:") #connection to sql
dbWriteTable(con,"funda",funda,overwrite=TRUE)
dbWriteTable(con,"msf",msf,overwrite=TRUE)
command <- "SELECT tsymbol,ret
FROM msf
WHERE date BETWEEN '2005-01-01' AND '2013-12-31'
AND tsymbol IN ('AAPL','SPY')"
result<-dbGetQuery(con,command)
result
result<-dbGetQuery(con,command)
y<-result[result$tsymbol=='AAPL',]$ret
x<-result[result$tsymbol=='SPY',]$ret
cov(x,y)/var(x)
lmout <- summary(lm(y~x+1))
lmout
shapiro.test(x)
?shapiro.test
plot(x,y)
names(lmout)
lmout$coefficients
library(quantmod)
install.packages("quantmod")
library(quantmod)
install.packages("quantmod")
library(quantmod)
getSymbols("F", from="2015-01-01",to="2017-01-01",periodicity="weekly",return.class="ts")
getSymbols("BABA", from="2015-01-01",to="2017-01-01",periodicity="weekly",return.class="ts")
head(F)
head(BABA)
mydat<-cbind(F[,6], BABA[,6])
mydat
colnames(mydat)<-c("F","BABA")
mydat
def <- c(10,25,33,29,18,12,23,27,30) #degrees freedom Vector
par(mfrow=c(3,3)) # 9 graphs per page
for(i in 1:length(def)){ # loop through all 9 degrees freedom
tdat=rt(100,def[i]) #generate 100 random value which is from a t-distribution
hist(tdat, breaks=100, main = paste("Histogram df = ", def[i])) #histogram
plot(density(tdat), main = paste("Density Curve df = ", def[i])) #density curve non - parametric
plot(dt(tdat,def[i]),type = "l", main = paste("Density Function df = ", def[i])) # density value parametric fit to distribution
}
1000/(1.02)^30
1000/(1.03)^30
data(google)
plot(google,col='blue')
price <- exp(cumsum(google)) * 50.12 #we get return convert them into price data
plot(price,type='l',col='blue')
library(quantmod)
library(PerformanceAnalytics)
data(google)
plot(google,col='blue')
price <- exp(cumsum(google)) * 50.12 #we get return convert them into price data
plot(price,type='l',col='blue') #price is random walk but return if it was rising would need to be trnasformed
library(TSA)
library(ggplot2)
data(google)
plot(google,col='blue')
price <- exp(cumsum(google)) * 50.12 #we get return convert them into price data
plot(price,type='l',col='blue') #price is random walk but return if it was rising would need to be trnasformed
hist(google,breaks=100)
hist(google,breaks=100)
sum(abs(google)>0.06)#from the histogram we see some extreme values rerurn the test with >.06 abs removed
shapiro.test(google)
sum(abs(google)>0.06)#
shapiro.test(google[abs(google)>0.06])
shapiro.test(google[abs(google)<= 0.06])
par(mfrow=c(2,2))
acf(google)
pacf(google)
acf(google^2) #when we square it we see that the conditional variance show some time dependence
pacf(google^2)
mean(google)
t.test(google, alternative='greater')
par(mfrow=c(1,2))
McLeod.Li.test(y=google)
McLeod.Li.test(y=rnorm(500))
?eacf
eacf(google^2)
m1 <- garch(x=google-mean(google),order=c(1,1),reltol=1e-6)
summary(m1)
plot(residuals(m1),type='h',ylab='standard residuals',col='blue')
par(mfrow=c(3,1))
plot(price,type='l',col='blue',ylab='price')
plot(google,type='l',col='blue',ylab='log returns')
plot((fitted(m1)[,1])^2,type='l',
ylab='conditional variance',xlab='time',col='blue') #estimated or perdicted conditional variance
m1 <- garch(x=google-mean(google),order=c(1,1),reltol=1e-6)
summary(m1)
plot(residuals(m1),type='h',ylab='standard residuals',col='blue')
par(mfrow=c(3,1))
plot(price,type='l',col='blue',ylab='price')
plot(google,type='l',col='blue',ylab='log returns')
plot((fitted(m1)[,1])^2,type='l',
ylab='conditional variance',xlab='time',col='blue') #estimated or perdicted conditional variance
m1 <- garch(x=google-mean(google),order=c(1,1),reltol=1e-6)
summary(m1)
#Chapter VI
library(quantmod)
library(PerformanceAnalytics)
library(TSA)
library(tseries
m1 <- garch(x=google-mean(google),order=c(1,1),reltol=1e-6)
summary(m1
summary(m1)
library(quantmod)
library(PerformanceAnalytics)
library(TSA)
library(ggplot2)
library(TSA)
library(tseries)
hist(google,breaks=100)
sum(abs(google)>0.06)
shapiro.test(google)
data(google)
plot(google,col='blue')
price <- exp(cumsum(google)) * 50.12 #we get return convert them into price data
plot(price,type='l',col='blue') #price is random walk but return if it was rising would need to be trnasformed
#GAUGE
hist(google,breaks=100)
sum(abs(google)>0.06)
shapiro.test(google)
par(mfrow=c(2,2))
acf(google)
pacf(google)
acf(google^2) #when we square it we see that the conditional variance show some time dependence
pacf(google^2)
mean(google) #need mean in your garch model see garch attributes, only focus on deviations
t.test(google, alternative='greater')
par(mfrow=c(1,2))
McLeod.Li.test(y=google)
McLeod.Li.test(y=rnorm(500))
eacf(google^2)
m1 <- garch(x=google-mean(google),order=c(1,1),reltol=1e-6)
summary(m1)
plot(residuals(m1),type='h',ylab='standard residuals',col='blue')
par(mfrow=c(3,1))
plot(price,type='l',col='blue',ylab='price')
plot(google,type='l',col='blue',ylab='log returns')
plot((fitted(m1)[,1])^2,type='l',
ylab='conditional variance',xlab='time',col='blue') #estimated or perdicted conditional variance
par(mfrow=c(2,2))
plot(residuals(m1),col="blue",main="Residuals")
hist(residuals(m1))
McLeod.Li.test(y=residuals(m1),main="McLeod-Li")
qqnorm(residuals(m1),col='blue')
qqline(residuals(m1))
shapiro.test(residuals(m1))
var(google)
library(tseries)
tmixture <- function(N,sigma1,sigma2=0,sigma3=0)
#three level mixture with state changes
{
variates = vector(length=N)
mode = 1
B = rbinom(365,1,1/365)
for(i in 1:N)
variates[i] = rnorm(1,0,sd=sigma1)
if(sigma2 != 0) { #only mixture if sigma2 != 0
for(i in 1:N)
if(B[i] == 1) {
mode = 2
#replace original variate with mixture variate
variates[i] = rnorm(1,0,sd=sigma2)
print(sigma2)
print(variates[i])
} else if (mode == 2) {
variates[i] = rnorm(1,0,sd=sigma3)
}
}
variates
}
install.packages("tseries")
library(tseries)
setwd(paste(homeuser,"/FinAnalytics/ChapV",sep=""))
S<-rev(read.csv("CHFperEUR.csv",header=TRUE)[,2])
library(Quandl)
S2<-1/rev(Quandl('ECB/EURCHF',
start_date="2014-01-30",end_date="2015-01-29")[,2])
par(mfrow=c(2,2))
diffLogS <- diff(log(S))
plot(diffLogS,type='p',ylim=c(-.08,.08))
plot(S,type='l',col='blue',ylim=c(.60,1.05),
xlab="One Year: early 2014 - early 2015",
ylab="actual CHF per EUR")
S[351:359]
diffLogS351 <- diff(log(S[1:351]))
diffLogS351mean <- mean(diffLogS351)
diffLogS351mean
diffLogS351dailyVol <- sd(diffLogS351)
diffLogS351dailyVol
diffLogSjumpMean = mean(diff(log(S[351:353])))
sd(diff(log(S[351:353])))/diffLogS351dailyVol
diffLogSlast <- diff(log(S[355:365]))
sd(diffLogSlast)/diffLogS351dailyVol
diffLogS351 <- diff(log(S[1:351]))
diffLogS351mean <- mean(diffLogS351)
diffLogS351mean
diffLogS351dailyVol <- sd(diffLogS351)
diffLogS351dailyVol
diffLogSjumpMean = mean(diff(log(S[351:353])))
sd(diff(log(S[351:353])))/diffLogS351dailyVol
diffLogSlast <- diff(log(S[355:365]))
sd(diffLogSlast)/diffLogS351dailyVol
b = 196
for(path in b:205) {
N=365
set.seed(path)
Y <- tmixture(N,diffLogS351dailyVol,
73.00818*diffLogS351dailyVol,
17.84*diffLogS351dailyVol)
if(path == b)
plot(Y,ylim=c(-.08,.08),xlab=path)
Yprices = c(S[[1]],S[[1]]*exp(cumsum(Y)))
if(path == b)
plot(Yprices,col=14,type="l",ylim=c(.60,1.05),
xlab="365 Days = 1 Simulated Year",
ylab="simulated CHF per EUR")
else
lines(Yprices,col=mapToCol(path%%24))
print(path)
Sys.sleep(5)
}
pruneBySharpe <- function(prices,lab,meanv,sdevv,threshSR,mufree=0) {
par(mar=c(4,4,1,1))
par(mfrow=c(1,2))
indepSharpes <- (meanv-mufree)/sdevv
len = length(indepSharpes)
plot(indepSharpes,ylab="SR",col=4)
plot(sort(indepSharpes),ylab="SR",col=4)
lines(1:len,rep(threshSR,len))
indHighSharpes <- (indepSharpes > threshSR)
#clean up NAs
for(d in 1:length(indHighSharpes)) #clean up NAs
if(is.na(indHighSharpes[d]))
indHighSharpes[d] <- FALSE
len = dim(prices)[1]
wid = dim(prices)[2]
smallerSz = sum(indHighSharpes)
newPrices <- matrix(rep(0,len*smallerSz),
nrow=len,ncol=smallerSz)
newLab    <- vector(length=smallerSz)
e <- 1
for(d in 1:wid) {
if(indHighSharpes[d]) {
print(paste("e",e))
newPrices[,e] <- prices[,d]
newLab[e] <- lab[d]
e <- e + 1
}
}
print("completed Sharpe pruning")
list(newPrices,newLab,indepSharpes)
}
library(huge)
data(stockdata)
D <- length(stockdata$data[1,])
p <- stockdata$data[,1:D]
l <- stockdata$info[1:D,1]
#Group 2
#Jiemin
#Jessie
#Johnson
#Rutuja
library(quantmod)
library(PerformanceAnalytics)
tickers=c('F','BABA','CE','T','O','BAC','AAPL','CAT','JPM','EL',
'ICE','KO','MMM','NVDA','NKE','MSFT','F','SAFT','UTX','AMZN','^GSPC','^IRX')
for (i in tickers){
getSymbols(i, from="2017-01-01",to="2019-01-01",periodicity="weekly",return.class="ts")
}
sp500=GSPC[,6]
tbill=IRX[,6]
data1 <-cbind(F[,6],BABA[,6],CE[,6],T[,6],O[,6],BAC[,6],AAPL[,6],CAT[,6],JPM[,6],EL[,6],
ICE[,6],KO[,6],MMM[,6],NVDA[,6],NKE[,6],MSFT[,6],F[,6],SAFT[,6],UTX[,6],AMZN[,6])
nameStock <-c('F','BABA','CE','T','O','BAC','AAPL','CAT','JPM','EL',
'ICE','KO','MMM','NVDA','NKE','MSFT','F','SAFT','UTX','AMZN')
colnames(data1)=nameStock
dim(data1)
View(data1)
#1.
diffLogs<-diff(log(data1))
logReturns=diffLogs
dim(diffLogs)
excessReturn <- logReturns - diff(log(tbill))
dim(excessReturn)
#2.
cor=cor(logReturns)
cor
cov=cov(logReturns)
cov
#3.
summR = summary(logReturns)
summR
skewness(logReturns)
kurtosis(logReturns)
#4.
#run the pruneBySharpe funciton first
pruneBySharpe <- function(prices,lab,meanv,sdevv,threshSR,mufree=0) {
par(mar=c(4,4,1,1))
par(mfrow=c(1,2))
indepSharpes <- (meanv-mufree)/sdevv
len = length(indepSharpes)
plot(indepSharpes,ylab="SR",col=4,xlab="Before sorting")
plot(sort(indepSharpes),ylab="SR",col=4,xlab="After sorting")
lines(1:len,rep(threshSR,len),col="red") #lines(x value,y value)
#abline(h=threshSR,col="red",lty=2)
indHighSharpes <- (indepSharpes > threshSR) #indHighSharpes means indicator sharpe ratio
#clean up NAs
for(d in 1:length(indHighSharpes)) #clean up NAs
if(is.na(indHighSharpes[d]))
indHighSharpes[d] <- FALSE
len = dim(prices)[1]
wid = dim(prices)[2]
smallerSz = sum(indHighSharpes) #smallerSz is the smaller size. It is the reduced size after selected
newPrices <- matrix(rep(0,len*smallerSz),
nrow=len,ncol=smallerSz)
newLab    <- vector(length=smallerSz)
e <- 1
for(d in 1:wid) {
if(indHighSharpes[d]) {
print(paste("e",e))
newPrices[,e] <- prices[,d]
newLab[e] <- lab[d]
e <- e + 1
}
}
print("completed Sharpe pruning")
list(newPrices,newLab,indepSharpes)
}
means=apply(logReturns,2,mean)
sdevv=(diag(cov))^0.5
#threshSR equal to 0.05 to select the top10 sharpe-ratio stocks (half of the total of 20 stocks)
data2 = pruneBySharpe(data1,nameStock,means,sdevv,threshSR=.05,mufree=mean(diff(log(tbill)))/52)
#5.
selectStock=data2[[2]]
selectStock
length(selectStock)
selectStock.Price=data2[[1]]
colnames(selectStock.Price)=selectStock
head(selectStock.Price)
dim(selectStock.Price)
selectStock.Return=diff(log(selectStock.Price))
head(selectStock.Return)
dim(selectStock.Return)
new.cor=cor(selectStock.Return)
new.cov=cov(selectStock.Return)
new.cor
new.cov
dim(new.cor)
dim(new.cov)
#6. We created a portfolio with each selected stock equal-weighted
weight=c(rep(1/10,10))
ourPortfolio.return=selectStock.Return %*% weight
colnames(ourPortfolio.return)=c("Return of Our Portfolio")
head(ourPortfolio.return)
dim(ourPortfolio.return)
#7.
#compute expected return of our portfolio
expected.Port.Return=mean(ourPortfolio.return)
expected.Port.Return #This is weekly return
Annualized.expected.Port.Return= expected.Port.Return*52
Annualized.expected.Port.Return # yearly return
Daily.expected.Port.return=Annualized.expected.Port.Return/252
Daily.expected.Port.return #daily return
#compute standard deviation of our portfolio
Port.standard.deviation=sd(ourPortfolio.return)
Port.standard.deviation #weekly standard deviaton
Annualized.Port.sd=Port.standard.deviation*52^0.5
Annualized.Port.sd
Daily.Port.sd=Annualized.Port.sd*(1/252)^0.5
Daily.Port.sd
weekly.riskfree.rate=mean(diff(log(tbill))/52)
weekly.riskfree.rate
sharpe.ratio= (expected.Port.Return-weekly.riskfree.rate)/Port.standard.deviation
sharpe.ratio #weekly sharpe ratio
Annualized.sharp.ratio=sharpe.ratio*52
Annualized.sharp.ratio
#8.Examine the portfolio's performance
#Our porfolio weekly return is 0.36% compared to SP500 weekly return of 0.09%.It shows
#our portfolio outperformed the market benchmark return from 2017 to 2019
print(c(expected.Port.Return,mean(diff(log(sp500)))))
par(mfrow=c(1,1))
hist(ourPortfolio.return,density = T,breaks=15)
lines(density(ourPortfolio.return),col="red")
skewness(ourPortfolio.return) #portfolio return is left-skewed, indicating more negative returns
kurtosis(ourPortfolio.return) # kurtosis is less than 3, showing no extreme returns during the observing period
rmixture <- function(N,sigma1,sigma2=0,thresh=.9) {
variates = vector(length=N)
U = runif(N)
for(i in 1:N)
variates[i] = rnorm(1,0,sd=sigma1)
if(sigma2 != 0) { #only mixture if sigma2 != 0
for(i in 1:N)
if(U[i] >= thresh)
#replace original variate with mixture variate
variates[i] = rnorm(1,0,sd=sigma2)
}
variates
}
toPrices=function(Y1,Ylogrets){
Yprices = c(Y1,Y1*exp(cumsum(Ylogrets)))
Yprices
}
simPricePath <- function(initPrice,N,seed,sigma1=.05,
sigma2=0,thresh=.9) {
#Non mixture model
set.seed(seed)
Xlogrets = rmixture(N,sigma1,sigma2,thresh=thresh)
Xprices = toPrices(initPrice,Xlogrets)
list(Xprices,c(Xlogrets))
}
# Perform one-month simulation for our portfolio using portfolio daily sigma (30 observations in a month generated)
oneMonth.sim=simPricePath(initPrice=10000,N=30,seed=1,sigma1=Daily.Port.sd,sigma2=0)
oneMonth.sim
# Perform three-month simulation for our portfolio using daily sigma (30*3 observations in a month generated)
threeMonth.sim=simPricePath(initPrice=10000,N=30*3,seed=1,sigma1=Daily.Port.sd,sigma2=0)
threeMonth.sim
#9.
hist(oneMonth.sim[[2]],density = T,main="Simulation of Returns in One Month",xlab = "Simulated Return")
hist(threeMonth.sim[[2]],density= T,main="Simulation of Returns in Three Month",xlab = "Simulated Return")
# 95% VaR of the portfolio over one month and three months separately
VaR.OneMonth=quantile(oneMonth.sim[[2]],probs = 0.05)
VaR.OneMonth
VaR.ThreeMonth=quantile(threeMonth.sim[[2]],probs = 0.05)
VaR.ThreeMonth
#conclusions:
#The 95% VaR in the one month simulation is -1.29%, indicating that there is 95% probility that
#our portfolio return would not exceed -1.29%.
#The 95% VaR in the three month simulation is -1.12%, suggesting that there is 95% probility that
#our portfolio return would not exceed -1.12%.
